# -*- coding: utf-8 -*-
"""Copy of CH3-10-12-2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sKtbA6mWhNNYDDy9HwlprIH17_cLqR0u
"""

text = """Dr. Ananya Verma, a legendary Indian physicist born in Kolkata in 1960, has made significant contributions to theoretical physics. Her achievements include the discovery of the Bose–Verma condensate, formulation of quantum entanglement theorems, and resolution of the Verma–Hawking black hole paradox. Dr. Verma’s brilliance has earned her praise from fellow scientists, and her legacy continues to inspire generations of physicists worldwide."""

from sentence_transformers import SentenceTransformer,util
import numpy as np
import nltk
from nltk.tokenize import sent_tokenize

nltk.download('punkt')
nltk.download('punkt_tab')

#chunking
sentences = sent_tokenize(text)
chunks =[' '.join(sentences[i:i+2]) for i in range(0,len(sentences),2)]
print("Auto generated chunks are:")
print(chunks)

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') #nvidia/Nemotron-Orchestrator-8B

chunk_embeddings = model.encode(chunks,convert_to_tensor=True)

#embed the question
question = "what are discoveries by Dr. Ananya Verma?"
question_embedding = model.encode(question,convert_to_tensor=True)

cosine_scores = util.cos_sim(question_embedding,chunk_embeddings)[0]
top_k = 2
top_indices = np.argsort(-cosine_scores.cpu().numpy())[:top_k]
relevangt_chunks = [chunks[i] for i in top_indices]
print(relevangt_chunks)

context = "\n".join(relevangt_chunks)
print(context)

final_prompt = f"""Context:{context}
          "question":{question}
          Answer :"""
print(final_prompt)

from dotenv import load_dotenv
import os
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

from google.colab import userdata
userdata.get('OPENAI_API_KEY')

import openai
from google.colab import userdata

# Get the API key directly from Colab's userdata
api_key_from_colab = userdata.get('OPENAI_API_KEY')

# Initialize the OpenAI client by passing the API key directly
client = openai.OpenAI(api_key=api_key_from_colab)

response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
      {"role":'system',"content":"you are a helpful assistant"},
      {"role":"user","content":final_prompt}
  ]
)
answer = response.choices[0].message.content.strip()
print(answer)

from datasets import load_dataset

ds = load_dataset("Anthropic/AnthropicInterviewer")



"""### **Explanation for Code Cell `6DRsT2V65rt8`**

```python
text = '''Dr. Ananya Verma, a legendary Indian physicist born in Kolkata in 1960, has made significant contributions to theoretical physics. Her achievements include the discovery of the Bose–Verma condensate, formulation of quantum entanglement theorems, and resolution of the Verma–Hawking black hole paradox. Dr. Verma’s brilliance has earned her praise from fellow scientists, and her legacy continues to inspire generations of physicists worldwide.'''
```
*   `text = '''...'''`: Initializes a multiline string variable named `text`. This string contains the raw biographical information about Dr. Ananya Verma, which serves as our primary data source for the NLP tasks in this notebook.

### **Explanation for Code Cell `dY6NHsKs6Jsp`**

```python
from sentence_transformers import SentenceTransformer,util
import numpy as np
import nltk
from nltk.tokenize import sent_tokenize
```
*   `from sentence_transformers import SentenceTransformer,util`: Imports `SentenceTransformer` (for creating text embeddings) and `util` (for utility functions like cosine similarity) from the `sentence_transformers` library.
*   `import numpy as np`: Imports the `numpy` library, commonly aliased as `np`, which is essential for numerical operations, especially with multi-dimensional arrays (vectors and matrices).
*   `import nltk`: Imports the Natural Language Toolkit (NLTK) library, a foundational tool for various natural language processing tasks.
*   `from nltk.tokenize import sent_tokenize`: Specifically imports the `sent_tokenize` function from NLTK's `tokenize` module, used for splitting text into individual sentences.

### **Explanation for Code Cell `HX6ANw6v6bgX`**

```python
nltk.download('punkt')
nltk.download('punkt_tab')
```
*   `nltk.download('punkt')`: Downloads the 'punkt' tokenizer models, which are required by NLTK's `sent_tokenize` function to accurately identify sentence boundaries in text.
*   `nltk.download('punkt_tab')`: Downloads additional tokenizer data, potentially for improved or specialized sentence tokenization capabilities (e.g., language-specific rules) beyond the basic 'punkt' model.

### **Explanation for Code Cell `bxL7poJM6jdP`**

```python
#chunking
sentences = sent_tokenize(text)
chunks =[' '.join(sentences[i:i+2]) for i in range(0,len(sentences),2)]
print("Auto generated chunks are:")
print(chunks)
```
*   `#chunking`: A comment indicating the logical section for text chunking.
*   `sentences = sent_tokenize(text)`: Splits the `text` variable into a list of individual sentences using the NLTK `sent_tokenize` function.
*   `chunks =[' '.join(sentences[i:i+2]) for i in range(0,len(sentences),2)]`: Creates `chunks` by iterating through the `sentences` list and grouping every two consecutive sentences together, joining them with a space.
*   `print("Auto generated chunks are:")`: Prints a descriptive string to the console.
*   `print(chunks)`: Displays the list of generated text chunks.

### **Explanation for Code Cell `hem4n0X566e9`**

```python
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') #nvidia/Nemotron-Orchestrator-8B
```
*   `model = SentenceTransformer(...)`: Initializes and loads a pre-trained `SentenceTransformer` model. The specific model `'sentence-transformers/all-MiniLM-L6-v2'` is chosen for its efficiency and effectiveness in generating high-quality sentence embeddings. This model converts text into numerical vectors that capture semantic meaning.

### **Explanation for Code Cell `oQIfYBO77GQI`**

```python
chunk_embeddings = model.encode(chunks,convert_to_tensor=True)
```
*   `chunk_embeddings = model.encode(chunks,convert_to_tensor=True)`: Uses the loaded `model` to convert each text `chunk` from the `chunks` list into its corresponding numerical vector (embedding). The `convert_to_tensor=True` argument ensures these embeddings are returned as a PyTorch tensor, suitable for further numerical computations.

### **Explanation for Code Cell `chmGJhYt7Qli`**

```python
#embed the question
question = "what are discoveries by Dr. Ananya Verma?"
question_embedding = model.encode(question,convert_to_tensor=True)
```
*   `#embed the question`: A comment indicating that the following code pertains to embedding the user's question.
*   `question = "what are discoveries by Dr. Ananya Verma?"`: Defines the specific question as a string that we want to answer based on the provided text.
*   `question_embedding = model.encode(question,convert_to_tensor=True)`: Encodes the `question` string into a numerical vector (embedding) using the same `SentenceTransformer` `model`. This allows for direct comparison with the `chunk_embeddings` in the same vector space.

### **Explanation for Code Cell `MyMXfViD7nia`**

```python
cosine_scores = util.cos_sim(question_embedding,chunk_embeddings)[0]
top_k = 2
top_indices = np.argsort(-cosine_scores.cpu().numpy())[:top_k]
elevangt_chunks = [chunks[i] for i in top_indices]
print(relevangt_chunks)
```
*   `cosine_scores = util.cos_sim(question_embedding,chunk_embeddings)[0]`: Calculates the cosine similarity between the `question_embedding` and each of the `chunk_embeddings`. Cosine similarity measures the angle between two vectors, indicating their semantic relatedness.
*   `top_k = 2`: Sets a variable `top_k` to `2`, specifying that we want to retrieve the two most relevant chunks.
*   `top_indices = np.argsort(-cosine_scores.cpu().numpy())[:top_k]`: Converts the cosine scores to a NumPy array, negates them to sort in descending order of similarity, and then gets the indices of the `top_k` highest scores.
*   `relevangt_chunks = [chunks[i] for i in top_indices]`: Uses the `top_indices` to extract the actual text content of the most relevant chunks from the original `chunks` list.
*   `print(relevangt_chunks)`: Displays the list of text chunks identified as most relevant to the question.

### **Explanation for Code Cell `uHO3Q3Uj8QJV`**

```python
context = "\n".join(relevangt_chunks)
print(context)
```
*   `context = "\n".join(relevangt_chunks)`: Joins the list of `relevangt_chunks` into a single string. Each chunk is separated by a newline character (`\n`), forming the consolidated `context` that will be provided to the Large Language Model.
*   `print(context)`: Prints the final `context` string to the console.

### **Explanation for Code Cell `z019sL4U8dUf`**

```python
final_prompt = f'''Context:{context}
          "question":{question}
          Answer :'''
print(final_prompt)
```
*   `final_prompt = f'''...'''`: Constructs the `final_prompt` as an f-string, embedding the previously generated `context` and the original `question`. The structure explicitly labels the context, question, and an anticipated 'Answer', guiding the LLM's response generation.
*   `print(final_prompt)`: Displays the complete `final_prompt` string that will be sent to the LLM.

### **Explanation for Code Cell `52lQpTDC8y_8`**

```python
from dotenv import load_dotenv
import os
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
```
*   `from dotenv import load_dotenv`: Imports the `load_dotenv` function from the `dotenv` library.
*   `import os`: Imports Python's built-in `os` module, which provides a way to interact with the operating system, including accessing environment variables.
*   `load_dotenv()`: Calls the `load_dotenv` function to load key-value pairs from a `.env` file (if present in the current or parent directories) into the system's environment variables.
*   `OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")`: Retrieves the value of the environment variable named `OPENAI_API_KEY` and assigns it to a Python variable.

### **Explanation for Code Cell `IeskTdrbNbaY`**

```python
from google.colab import userdata
userdata.get('OPENAI_API_KEY')
```
*   `from google.colab import userdata`: Imports the `userdata` module, a Google Colab-specific utility for securely managing and accessing user secrets.
*   `userdata.get('OPENAI_API_KEY')`: Retrieves the value associated with the secret named `OPENAI_API_KEY` from Google Colab's integrated secrets manager.

### **Explanation for Code Cell `nVEp9NRV9FcZ`**

```python
import openai
from google.colab import userdata

# Get the API key directly from Colab's userdata
api_key_from_colab = userdata.get('OPENAI_API_KEY')

# Initialize the OpenAI client by passing the API key directly
client = openai.OpenAI(api_key=api_key_from_colab)

response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
      {"role":'system',"content":"you are a helpful assistant"},
      {"role":"user","content":final_prompt}
  ]
)
answer = response.choices[0].message.content.strip()
print(answer)
```
*   `import openai`: Imports the official OpenAI Python client library.
*   `from google.colab import userdata`: Imports `userdata` for securely accessing Colab secrets.
*   `api_key_from_colab = userdata.get('OPENAI_API_KEY')`: Retrieves the OpenAI API key from Colab's secrets manager.
*   `client = openai.OpenAI(api_key=api_key_from_colab)`: Initializes an instance of the `OpenAI` client, passing the securely retrieved API key for authentication.
*   `response = client.chat.completions.create(...)`: Makes an API call to OpenAI's chat completions endpoint to generate a response.
*   `model="gpt-3.5-turbo"`: Specifies the particular Large Language Model to use for generating the response.
*   `messages=[...]`: Provides the input conversation history to the LLM. It includes a 'system' message (setting the AI's persona) and a 'user' message (containing our `final_prompt` with context and question).
*   `answer = response.choices[0].message.content.strip()`: Extracts the actual generated text response from the API call's return object. `.strip()` removes leading/trailing whitespace.
*   `print(answer)`: Displays the LLM's final generated answer to the console.

### **Explanation for Code Cell `1FwrqPtV94xl`**

```python
from datasets import load_dataset

ds = load_dataset("Anthropic/AnthropicInterviewer")
```
*   `from datasets import load_dataset`: Imports the `load_dataset` function from the Hugging Face `datasets` library.
*   `ds = load_dataset("Anthropic/AnthropicInterviewer")`: Downloads and loads a specific dataset named "Anthropic/AnthropicInterviewer" from the Hugging Face Hub into a variable `ds`. This demonstrates how to easily access public machine learning datasets.

## GitHub Repository Summary: Retrieval Augmented Generation (RAG) Explained

This notebook demonstrates a foundational **Retrieval Augmented Generation (RAG)** system using readily available Python libraries. It walks through the process of intelligently answering questions based on a given text corpus, significantly enhancing the factual accuracy and relevance of AI-generated responses.

### Key Features & Concepts Covered:

*   **Text Preprocessing:** Utilizes `nltk` for sentence tokenization and custom chunking to break down raw text into manageable, semantically coherent segments.
*   **Sentence Embeddings:** Employs `sentence-transformers` (specifically `all-MiniLM-L6-v2`) to convert text chunks and user queries into dense numerical vectors (embeddings), capturing their semantic meaning.
*   **Semantic Search:** Implements cosine similarity to perform efficient semantic search, identifying and retrieving the most relevant text chunks from the corpus based on a user's question.
*   **Prompt Engineering:** Constructs a structured prompt by integrating the retrieved relevant context with the user's original question, guiding a Large Language Model (LLM) to generate accurate and grounded answers.
*   **LLM Integration:** Demonstrates interaction with the OpenAI API (`gpt-3.5-turbo`) to synthesize responses using the pre-processed and semantically matched context.
*   **Secure API Key Management:** Illustrates best practices for handling sensitive credentials using Google Colab's `userdata` secrets manager.
*   **Dataset Loading:** Briefly shows how to load datasets from the Hugging Face Hub, highlighting the accessibility of ML resources.

### Why this approach?

RAG addresses a core challenge in generative AI: preventing 'hallucinations' and grounding LLM responses in verifiable facts. By first retrieving relevant information and then augmenting the LLM's prompt with this data, the system ensures more accurate, reliable, and contextually appropriate outputs, making it ideal for building robust question-answering systems and intelligent assistants.
"""